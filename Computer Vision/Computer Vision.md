# 图像的基本表示

图像的张量表示: (高, 宽, 通道)

通道可以为1, 3, 4(2一般是灰度通道, 3一般是RGB或者HSV, 4一般是CMYK或者RGBA) , 每个通道都有一个高*宽的矩阵组成, 每个像素点上的值在0-255之间, 表示对应的灰度
 
*注意opencv在读取图像时默认的通道顺序是BGR*, 需要进行手动转换
```python
img = cv2.cvtColor(img, 
                   cv2.COLOR_BGR2RGB)  # 转换方式
```
一些基本理解:
1.   给图像加上某个数值, 就会让图像变亮
2.   给图像减去某个数值, 就会让图像变暗
3.   给图像乘上某个数值, 就会让图像变鲜艳(调整对比度)

# 卷积和卷积神经网络

## 卷积
二维矩阵的卷积表示其中一个矩阵在平面上旋转180度后, 与另一个矩阵求**点积**的结果(卷 = 旋转, 积 = 点积)

但是实际应用中, 我们不再进行旋转这一步, 直接进行点积操作

我们把旋转后的矩阵的值称为权重, 把该矩阵称为**过滤器**, 也叫做**卷积核**

由于图像像素矩阵往往很大, 我们不可能创造一个和原图像素矩阵一样大的权重矩阵来进行卷积运算. 因此我们往往创造一个较小的矩阵, 然后对像素矩阵进行扫描: 
![alt text](image.png)
![alt text](image-1.png)
![alt text](image-2.png)

每个卷积核在原图上扫描的区域(即绿色区域)被称为**感受野**, 卷积核与感受野轮流点积得到的新矩阵被叫做**特征图**

使用不同的卷积核, 可以达到不同的效果, 比如边缘检测(索贝尔算子、拉普拉斯算子), 锐化, 模糊等

## 卷积神经网络
如何寻找效果更好的卷积核? 

深度学习的核心思想之一, 就是给算法训练目标, 让算法自己找最佳参数. 于是卷积神经网络就诞生了

### 参数共享
由于参数共享的性质, 卷积神经网络可以节省很大的参数量
*   **全连接DNN**: 假设图片尺寸为(600, 400), 输入DNN时需要将像素拉平至一维, 在输入层上就需要占用400*600=24w个神经元; 假设若干个隐藏层的神经元数为1w个, 则总共需要24亿个参数才能解决问题
*   **卷积神经网络CNN**: 无论图片多大, **卷积神经网络要求解的参数就是卷积核上的所有数字**. 假设卷积核的尺寸是5*5, 隐藏层的神经元个数为1w, 那么只需要求解25w个参数
  

### 稀疏交互
特征图上的一个神经元, 只和对应的感受野上极少数神经元有关. 因此, 神经元之间并不是全连接, 而是稀疏连接. 一般认为, 稀疏交互让CNN获得了提取更深特征的能力, 因为:
*   更符合数据的局部相关性假设
*   降低了过拟合风险